{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNiQbjHXmHM6"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
      ],
      "metadata": {
        "id": "EowPx6mrn27t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installs transformers, accelerate, and bitsandbytes libraries, which are essential for running and optimizing transformer models efficiently. The transformers library is used to work with pre-trained models, while accelerate and bitsandbytes help with faster computation and quantization. Finally, it imports AutoModelForCausalLM for language modeling and AutoTokenizer for tokenizing inputs."
      ],
      "metadata": {
        "id": "zY7BlfyZ7LbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPTMETA = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\")"
      ],
      "metadata": {
        "id": "E7fz4e4joCn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This loads the OPT-1.3b model from Hugging Face's Transformers library with 8-bit quantization enabled and stores it in a variable named OPTMETA. Quantization reduces the model's precision to save memory and improve efficiency when running on hardware like GPUs or CPUs. This is a memory-saving technique (8-bit precision) during the loading process.\n"
      ],
      "metadata": {
        "id": "ld6vxxvc1w-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizeropt = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")"
      ],
      "metadata": {
        "id": "ENPqST5Br064"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#inputval = \"In Bangalore it is a bright sunny\"\n",
        "inputval = \"A boy leaps across the sleeping\"\n",
        "\n",
        "inputval_tokenized = tokenizeropt(inputval, return_tensors=\"pt\")\n",
        "\n",
        "print(\"Token IDs size:\", inputval_tokenized['input_ids'].size())\n",
        "\n",
        "# Display each token and its corresponding token ID\n",
        "tokens = tokenizeropt.tokenize(inputval)\n",
        "ids = inputval_tokenized['input_ids'][0].tolist()  # Extract IDs from tensor\n",
        "token_id_pairs = tokenizeropt.convert_ids_to_tokens(ids)\n",
        "\n",
        "print(\"\\nTokens and Corresponding IDs:\")\n",
        "for token, token_id in zip(token_id_pairs, ids):\n",
        "    print(f\"Token: {token} - Token ID: {token_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwaRSwRIr6ra",
        "outputId": "478f0ef0-a041-45e4-f19a-bef585211177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs size: torch.Size([1, 7])\n",
            "\n",
            "Tokens and Corresponding IDs:\n",
            "Token: </s> - Token ID: 2\n",
            "Token: A - Token ID: 250\n",
            "Token: Ġboy - Token ID: 2143\n",
            "Token: Ġleaps - Token ID: 32564\n",
            "Token: Ġacross - Token ID: 420\n",
            "Token: Ġthe - Token ID: 5\n",
            "Token: Ġsleeping - Token ID: 8416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(OPTMETA.model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fy181uwAu2Gs",
        "outputId": "01bcd907-64b7-4aff-ef7e-224aa99c643d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OPTModel(\n",
            "  (decoder): OPTDecoder(\n",
            "    (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
            "    (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
            "    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "    (layers): ModuleList(\n",
            "      (0-23): 24 x OPTDecoderLayer(\n",
            "        (self_attn): OPTSdpaAttention(\n",
            "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "        )\n",
            "        (activation_fn): ReLU()\n",
            "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
            "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
            "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eplanation\n",
        "\n",
        "**embed_tokens:** Converts input words/tokens into vectors of size 2048 using a learned embedding matrix. (Vocabulary size = 50,272).\n",
        "\n",
        "**embed_positions:** Adds position information (like word order) to each token's vector so the model understands the sentence structure.\n",
        "\n",
        "The model has 24 layers of OPTDecoderLayer, which process the input data step-by-step.\n",
        "\n",
        "**Each decoder layer consists of:**\n",
        "\n",
        "Self-Attention (self_attn): Allows each word to \"focus\" on other words in the sentence.\n",
        "Projections (k_proj, v_proj, q_proj, out_proj): Map input into key, value, and query vectors.\n",
        "Linear8bitLt: These layers perform matrix multiplications but use 8-bit precision to save memory and improve efficiency.\n",
        "\n",
        "**Feedforward Network:**\n",
        "\n",
        "fc1: Expands the vector size from 2048 → 8192.\n",
        "fc2: Shrinks it back from 8192 → 2048.\n",
        "Activation Function (ReLU): Adds non-linearity so the model can learn complex patterns.\n",
        "\n",
        "**Layer Normalization (LayerNorm):** Normalizes outputs at different stages for stable training.\n",
        "\n",
        "The final layer normalizes the output one last time before producing the result.\n",
        "\n",
        "\n",
        "Together, these components enable the OPT model to process input tokens, understand relationships, and generate predictions efficiently."
      ],
      "metadata": {
        "id": "UK4jZlMjzqlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract token embeddings for the input tokens\n",
        "token_embeddings = OPTMETA.model.decoder.embed_tokens(inputval_tokenized['input_ids'])\n",
        "\n",
        "print(\"===== Token Embeddings Details =====\")\n",
        "print(f\"Layer:    {OPTMETA.model.decoder.embed_tokens}\")\n",
        "print(f\"Size:     {token_embeddings.size()}\")\n",
        "print(\"Output:   Token Embeddings (First 2 Tokens):\\n\", token_embeddings[:, :2, :])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS3vHhAD5Z3t",
        "outputId": "91a7fa0d-8697-4530-9652-0ec976780097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Token Embeddings Details =====\n",
            "Layer:    Embedding(50272, 2048, padding_idx=1)\n",
            "Size:     torch.Size([1, 7, 2048])\n",
            "Output:   Token Embeddings (First 2 Tokens):\n",
            " tensor([[[-0.0407,  0.0519,  0.0574,  ..., -0.0263, -0.0355, -0.0260],\n",
            "         [-0.0425,  0.0070,  0.0229,  ...,  0.0706, -0.0323, -0.0276]]],\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code extracts token embeddings for the input tokens using the embed_tokens layer of the OPT model. The embeddings have a size of (1, 9, 2048), which means there is 1 batch, 9 tokens, and each token is represented as a 2048-dimensional vector. The output shows the first 2 token embeddings (partial values) to keep the result readable. These embeddings are numerical representations of the tokens, enabling the model to understand the input text."
      ],
      "metadata": {
        "id": "2lFLDIzV6Lzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract positional embeddings for the input tokens\n",
        "positional_embeddings = OPTMETA.model.decoder.embed_positions(inputval_tokenized['attention_mask'])\n",
        "\n",
        "print(\"===== Positional Embeddings Details =====\")\n",
        "print(f\"Layer:    {OPTMETA.model.decoder.embed_positions}\")\n",
        "print(f\"Size:     {positional_embeddings.size()}\")\n",
        "print(\"Output:   Positional Embeddings (First 2 Positions):\\n\", positional_embeddings[:, :2, :])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkkEFLe3z9tQ",
        "outputId": "8fdbb4ab-900b-4ae7-e76b-a28c468bd010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Positional Embeddings Details =====\n",
            "Layer:    OPTLearnedPositionalEmbedding(2050, 2048)\n",
            "Size:     torch.Size([1, 7, 2048])\n",
            "Output:   Positional Embeddings (First 2 Positions):\n",
            " tensor([[[-8.1406e-03, -2.6221e-01,  6.0768e-03,  ...,  1.7273e-02,\n",
            "          -5.0621e-03, -1.6220e-02],\n",
            "         [-8.0585e-05,  2.5000e-01, -1.6632e-02,  ..., -1.5419e-02,\n",
            "          -1.7838e-02,  2.4948e-02]]], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code extracts **positional embeddings** for the input tokens using `embed_positions` from the OPT decoder.  \n",
        "`inputval_tokenized['attention_mask']` ensures embeddings align correctly with valid input tokens.  \n",
        "It prints the **layer** type (`OPTLearnedPositionalEmbedding`) and the **size** of the embeddings: `(1, 9, 2048)` → batch size 1, 9 positions, and 2048 dimensions.  \n",
        "It displays the **first two positional embeddings** (partial output) for simplicity.  \n",
        "The output tensor runs on a **GPU** (`cuda:0`) and uses **float16** precision for efficiency."
      ],
      "metadata": {
        "id": "fmuGcziC3nJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine token embeddings and positional embeddings\n",
        "combined_input = token_embeddings + positional_embeddings\n",
        "\n",
        "# Pass the combined input through the first self-attention layer\n",
        "hidden_states, _, _ = OPTMETA.model.decoder.layers[0].self_attn(combined_input)\n",
        "\n",
        "print(\"===== Self-Attention Layer Details =====\")\n",
        "print(f\"Layer:    {OPTMETA.model.decoder.layers[0].self_attn}\")\n",
        "print(f\"Size:     {hidden_states.size()}\")\n",
        "print(\"Output:   Hidden States (First 2 Positions):\\n\", hidden_states[:, :2, :])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntSKk9_G34ZV",
        "outputId": "a66b21e1-a2d2-49ec-8203-1d5c97958a34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Self-Attention Layer Details =====\n",
            "Layer:    OPTSdpaAttention(\n",
            "  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
            ")\n",
            "Size:     torch.Size([1, 7, 2048])\n",
            "Output:   Hidden States (First 2 Positions):\n",
            " tensor([[[-1.3504e-02, -9.5360e-03,  1.2638e-03,  ...,  6.4713e-03,\n",
            "          -1.7172e-03,  1.3433e-02],\n",
            "         [-1.2404e-02, -1.0845e-02,  1.1231e-03,  ...,  9.6275e-03,\n",
            "           8.4561e-05,  9.9411e-03]]], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code passes the combined input (token embeddings + positional embeddings) through the first self-attention layer of the OPT model's decoder. The self-attention layer allows each token to focus on other tokens, generating new hidden states that represent contextual relationships between tokens. The output shape (1, 9, 2048) indicates 1 batch, 9 positions (tokens), and 2048 dimensions per token. The displayed values show the first 2 hidden states, which are updated token representations after applying self-attention."
      ],
      "metadata": {
        "id": "nRXtczi76y_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the combined input through all decoder layers\n",
        "hidden_states = combined_input\n",
        "for i, layer in enumerate(OPTMETA.model.decoder.layers):\n",
        "    hidden_states, _, _ = layer.self_attn(hidden_states)\n",
        "    print(f\"Layer {i+1} Output Size: {hidden_states.size()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sxjJks3JMB6",
        "outputId": "1771a0d7-9d93-431c-d5b1-c56b1937fd2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 2 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 3 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 4 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 5 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 6 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 7 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 8 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 9 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 10 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 11 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 12 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 13 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 14 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 15 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 16 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 17 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 18 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 19 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 20 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 21 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 22 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 23 Output Size: torch.Size([1, 7, 2048])\n",
            "Layer 24 Output Size: torch.Size([1, 7, 2048])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_output = OPTMETA.model.decoder.final_layer_norm(hidden_states)\n",
        "print(\"===== Final Output Details =====\")\n",
        "print(f\"Size: {final_output.size()}\")\n",
        "print(\"Output (First 2 Positions):\\n\", final_output[:, :2, :])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0ziVzfdJozl",
        "outputId": "efbf6a35-e863-4e02-b1bc-5205523ee80f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Final Output Details =====\n",
            "Size: torch.Size([1, 7, 2048])\n",
            "Output (First 2 Positions):\n",
            " tensor([[[ 1.0349, -0.5554, -1.3153,  ...,  0.3701,  0.4754, -0.1551],\n",
            "         [ 1.0349, -0.5554, -1.3153,  ...,  0.3701,  0.4754, -0.1551]]],\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rZjj_xZK6uM",
        "outputId": "87d1f9ab-4682-40d0-dbeb-6dd45b5983ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Next Token:  body\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Generate 3 next tokens\n",
        "next_tokens = []\n",
        "current_input_ids = inputval_tokenized['input_ids']\n",
        "\n",
        "for _ in range(3):  # Loop for 3 predictions\n",
        "    # Compute logits using the updated input sequence\n",
        "    current_output = OPTMETA(**{\"input_ids\": current_input_ids}).logits\n",
        "\n",
        "    # Predict the next token ID\n",
        "    predicted_token_id = torch.argmax(current_output[:, -1, :], dim=-1)\n",
        "\n",
        "    # Decode the predicted token and store it\n",
        "    predicted_token = tokenizeropt.decode(predicted_token_id)\n",
        "    next_tokens.append(predicted_token)\n",
        "\n",
        "    # Append the predicted token ID to the current input IDs\n",
        "    current_input_ids = torch.cat((current_input_ids, predicted_token_id.unsqueeze(0)), dim=-1)\n",
        "\n",
        "# Print the predicted words\n",
        "print(f\"Predicted Next Tokens: {' '.join(next_tokens)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV-yd8TkLgNe",
        "outputId": "b6d42726-0280-4d57-b631-c2f6060fc7be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Next Tokens:  day  and  I\n"
          ]
        }
      ]
    }
  ]
}